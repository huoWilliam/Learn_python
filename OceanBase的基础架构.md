# OceanBase的基础架构

![image-20210527143456128](C:\Users\huo84\AppData\Roaming\Typora\typora-user-images\image-20210527143456128.png)

对于DBMS，它能看到的是三个集群里的若干台机器。而将这若干台机器管理起来，是分布式数据库要处理的第一件事情。

从上图中可以看出：

1. ==所有的节点（机器）都是对等的==。都运行着一个叫Observer的服务器程序。每个集群中有一台机器会被选取出来，作为RootService，它负责管理整个集群内的所有机器负载均衡之类的事务。[^1]
2. 用户的数据以分区的形式，分散存储在集群的所有机器上。一个表可能会被分成多个分区，存储在集群内不同的机器上。这是由分布式存储机制，将用户数据均匀的分割后存储在多台机器上。
3. 在此基础上，会建立存储引擎和SQL引擎，对外提供数据库的服务。

# 内核各模块介绍

接下来介绍DB各个模块的一些内部设计。

数据库内核最重要的三个组成部分：存储，事务，SQL引擎

## 存储引擎

存储引擎第一要务就是利用硬盘来存储用户的数据。

![image-20210527151516187](C:\Users\huo84\AppData\Roaming\Typora\typora-user-images\image-20210527151516187.png)

上图展示了OceanBase存储引擎架构。

从上图中可以看出：

右下部分，描述的是硬盘的数据存储区域。==它以块为单位==来存储用户表格内的数据。多个数据块共同组成了用户最终存储的数据。以块为单位来管理数据，几乎是所有数据库都会采用的策略。首先，这是因为硬盘本身是块存储设备；其次，以块为单位进行数据管理更加方便。

因为当前服务器的内存都很大。如何利用大内存，来帮助数据库更好地提供数据库服务，是OceanBase要解决的问题。对此，OceanBase采用了LSMTree的存储结构——在OceanBase中所有的数据修改不是就地进行的，而是先将所有的修改在内存中缓存起来。LSMTree是很多数据库存储引擎都会采用的存储结构，但是OceanBase有自己的特点。

1. 首先，OceanBase选择以2M为单位作为数据库的块。[^2]2M是比较大的块单位。为什么OceanBase为取这么大的块大小为单位？这是因为OceanBase基于一个有很大内存的假设——所有数据的修改，不会立即做数据块上的覆盖写的。所有数据修改内容先缓存在内存的一个数据结构MemTable里。这样带来的好处是，对同一个数据块的写操作被大大延缓，在一个更长的时间尺度里面，可以累积对同一数据块更多的修改，减少了写盘的次数。当然，从事务角度，数据的修改会先记入日志，保证故障时，可以通过日志恢复。
2. 将数据块做大以后，还可以带来更多的好处。在数据库这种标准化的表格里面存储的很多数据，其实是有很多重复的内容。比如，有时候很多列存储的都是同一日期，其实在实际存储时是没有必要存储所有日期的，可以对此进行数据压缩。这种数据压缩的技术其实是利用字典来做数据存储的方法。很多数据库都采用这种数据压缩的方式，但是在OceanBase的场景下，这种做法会显得更加的合理且取得效果最大化。因为，利用字典方式做数据存储时，最大的代价在于构造字典，即写数据时，把数据按照字典的方式进行分析，即哪些数据是重复的，哪些数据是新增的。这种分析以及建立数据字典的过程，要在写数据块的时候做。当数据块小的时候，写块的操作太频繁了，每次都做分析需要耗费大量的CPU。当数据块越大时，不仅数据压缩效果越好，节省了大量的存储空间。同时减少了访问CPU的次数。即以很小的CPU负担，获得大量的存储成本的节省。
3. 当数据存储在内存里时，数据的读取需要新的处理操作。OceanBase采用的方法就是在数据读取的时候要做实时的合并。首先，硬盘里的数据在内存中有缓存。读取数据时，要把内存里缓存的数据和内存里最新的修改数据做合并，得到最新的数据，返回给用户。

## 分布式事务

![image-20210527213947660](C:\Users\huo84\AppData\Roaming\Typora\typora-user-images\image-20210527213947660.png)

因为当数据库有了多台机器之后，不得不面对的问题是：一个用户要修改的数据可能分布在多台机器上。一台机器的事务提交和多台机器的事务提交的主要区别在于：如何保证事务的原子性？因为一台机器保证原子性是通过在该机器的日志系统里记录事务的Log就能搞定。但是多台机器时，就需要在多台机器上记录事务的Log。那么就需要引入类似于两阶段提交的方式来实现。

OceanBase对此的方案是：用户其实是不需要关注这个过程。他还是通过SQL来访问数据。当客户端连接到某台机器上之后，对于所有的SQL操作，OceanBase会自动的记录下用户所修改的地方。当用户要提交事务的时候，OceanBase会将本次事务中参与的所有机器统一起来，一起进行事务的提交。这个过程对用户而言是透明的。但是在数据库的内部会自动的分析，如果DB在一台机器上，就进行单机的事务提交系统。如果DB分布在多台机器上时，会自动进行两阶段提交。

当然对于两阶段提交如何保证提交过程不会卡死，OceanBase采用了很多高可用的策略。

### 高可用

![image-20210527215818069](C:\Users\huo84\AppData\Roaming\Typora\typora-user-images\image-20210527215818069.png)

因为分布式数据库系统自然而然地有了多台机器的计算能力和存储能力，很自然的我们把一份数据在多台机器上存储多份的时候，就必须保证足够的容灾能力。

同时，在多副本的情境下，如何保证副本之间的一致性也是非常重要的问题。OceanBase对此采用Paxos协议来保证一个集群内多个副本的数据始终保持一致。并且采用Paxos协议后，只要多数派的节点是始终在线的，那么不仅数据的安全得到保证，数据库的服务也能保证持续可用。

右下角是Oceanbase的Durability性能测试曲线。Durable测试要求就是要保证在有硬件故障的时候，数据的可靠性是不会受影响的。在OceanBase做测试时，TPCC的审计员就任意的杀掉两台机器，第一台机器是RootService节点，即管控节点。第二台机器是任意挑选一台数据库的服务节点杀掉。

杀掉服务节点时，出现了波动较大。因为数据库服务节点里连接了大量的客户端连接。当该节点被杀掉时，除了数据库本身的数据服务停止，客户端连接也会中断。此时，数据库内部立马重新选举一个新的副本，当作主副本，开始重新恢复数据服务，同时客户端连接重新开始连接。

整体上就是采用了Paxos协议，它既做到了数据的可靠性，有做到了数据的可用性，达到了两者完美的平衡。

## SQL引擎

![image-20210527222852655](C:\Users\huo84\AppData\Roaming\Typora\typora-user-images\image-20210527222852655.png)

OceanBase的SQL引擎是自主研发的，可以兼容MySQL和Oracle。可以使用MySQL的客户端连接OceanBase的服务器。

它不仅是自主研发的，而且还适配了OceanBase分布式的场景。实现了查询计划缓存（对于重复执行的SQL不需要重新生成执行计划），实现了并行执行（即可以同时利用多台机器上的计算资源和存储资源，来为SQL引擎提供服务），用JIT做编译执行，这些都OceanBase的独创。

# 兼容性

![image-20210527223946218](C:\Users\huo84\AppData\Roaming\Typora\typora-user-images\image-20210527223946218.png)

当OceanBase在走到越来越多的用户使用场景里，总会遇到大量的已有业务。他们以前或者使用MySql或者使用Oracle。当他们想转移到OceanBase时，该如何把他们的已有服务迁移过来，是OceanBase需要解决的。

[^1]: 但是它只是Observer中多出来的一个附属程序，其实任何一个Observer都可以干这件事情（每个机器内部会有一套多副本的机制，去保证服务的可用性和可靠性）。

[^2]: 不同块大小的影响？通常一个数据块里的任何内容有了修改，都需要对整块做修改。所以，数据块越小，修改的代价就越小。但是，如果数据块过小，又会带来管理代价的提升。如果数据块过大，又会导致数据修改的代价增大。于是，Orecle或MySql的数据块都取一个比较适当的大小，Orecle取8KB的块大小，MySql取16KB的块大小。

